{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GRU, Flatten, TimeDistributed\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from numpy import save\n",
    "from numpy import load as loadBinary\n",
    "import glob\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSummaryFile(filename):\n",
    "    \"\"\"\n",
    "    Create a summary file for the results of each gauge\n",
    "        \n",
    "    Parameters:\n",
    "        filename, str:\n",
    "            filename(path) of the summary file\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Results = {}\n",
    "    Results[\"Train\"] = {}\n",
    "    Results[\"Test\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"][\"max\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"][\"min\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"][\"median\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"][\"mean\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"][\"max\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"][\"min\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"][\"median\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"][\"mean\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"][\"max\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"][\"min\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"][\"median\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"][\"mean\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"][\"max\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"][\"min\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"][\"median\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"][\"mean\"] = {}\n",
    "    \n",
    "    with open(filename, \"w\") as outfile:\n",
    "        json.dump(Results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateJSON(station_id, NSE_train, NSE_test, KGE_train, KGE_test, jsonFilePath):\n",
    "    \"\"\"\n",
    "    Updates the summary file with results of a gauge\n",
    "    \n",
    "    Parameters:\n",
    "        station_id, int:\n",
    "            id of the gauge which results will be added to summary file\n",
    "        NSE_train, list:\n",
    "            NSE scores of train set\n",
    "        NSE_test, list:\n",
    "            NSE scores of test set\n",
    "        KGE_train, list:\n",
    "            KGE scores of train set\n",
    "        KGE_test, list:\n",
    "            KGE scores of test set\n",
    "        jsonFilePath, str:\n",
    "            path of the updated summary file\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    nse_train_max = np.max(NSE_train)\n",
    "    nse_train_min = np.min(NSE_train)\n",
    "    nse_train_median = np.median(NSE_train)\n",
    "    nse_train_mean = np.mean(NSE_train)\n",
    "    \n",
    "    nse_test_max = np.max(NSE_test)\n",
    "    nse_test_min = np.min(NSE_test)\n",
    "    nse_test_median = np.median(NSE_test)\n",
    "    nse_test_mean = np.mean(NSE_test)\n",
    "    \n",
    "    kge_train_max = np.max(KGE_train)\n",
    "    kge_train_min = np.min(KGE_train)\n",
    "    kge_train_median = np.median(KGE_train)\n",
    "    kge_train_mean = np.mean(KGE_train)\n",
    "    \n",
    "    kge_test_max = np.max(KGE_test)\n",
    "    kge_test_min = np.min(KGE_test)\n",
    "    kge_test_median = np.median(KGE_test)\n",
    "    kge_test_mean = np.mean(KGE_test)\n",
    "    \n",
    "    with open(jsonFilePath, \"r\") as jsonFile:\n",
    "        Results = json.load(jsonFile)\n",
    "    \n",
    "    Results[\"Train\"][\"NSE\"][\"max\"][station_id] = nse_train_max\n",
    "    Results[\"Train\"][\"NSE\"][\"min\"][station_id] = nse_train_min\n",
    "    Results[\"Train\"][\"NSE\"][\"median\"][station_id] = nse_train_median\n",
    "    Results[\"Train\"][\"NSE\"][\"mean\"][station_id] = nse_train_mean\n",
    "    \n",
    "    Results[\"Train\"][\"KGE\"][\"max\"][station_id] = kge_train_max\n",
    "    Results[\"Train\"][\"KGE\"][\"min\"][station_id] = kge_train_min\n",
    "    Results[\"Train\"][\"KGE\"][\"median\"][station_id] = kge_train_median\n",
    "    Results[\"Train\"][\"KGE\"][\"mean\"][station_id] = kge_train_mean\n",
    "    \n",
    "    Results[\"Test\"][\"NSE\"][\"max\"][station_id] = nse_test_max\n",
    "    Results[\"Test\"][\"NSE\"][\"min\"][station_id] = nse_test_min\n",
    "    Results[\"Test\"][\"NSE\"][\"median\"][station_id] = nse_test_median\n",
    "    Results[\"Test\"][\"NSE\"][\"mean\"][station_id] = nse_test_mean\n",
    "    \n",
    "    Results[\"Test\"][\"KGE\"][\"max\"][station_id] = kge_test_max\n",
    "    Results[\"Test\"][\"KGE\"][\"min\"][station_id] = kge_test_min\n",
    "    Results[\"Test\"][\"KGE\"][\"median\"][station_id] = kge_test_median\n",
    "    Results[\"Test\"][\"KGE\"][\"mean\"][station_id] = kge_test_mean\n",
    "    \n",
    "    with open(jsonFilePath, \"w\") as jsonFile:\n",
    "        json.dump(Results, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seg2Seg_main(station_id, resultDir, dataAsBinary):\n",
    "    \"\"\"\n",
    "    Main function to get results of Seq2Seq model for given gauge\n",
    "    \n",
    "    Parameters:\n",
    "        station_id, str:\n",
    "            id of gauge which model will use its training and test data\n",
    "        resultDir, str:\n",
    "            directory to store the results\n",
    "        dataAsBinary, str:\n",
    "            path to dataset which contains test and training data for each gauge as binary\n",
    "        \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def nse(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the NSE loss\n",
    "        \n",
    "        Parameters:\n",
    "            y_pred, array:\n",
    "                prediction results\n",
    "            y_true, array:\n",
    "                actual results\n",
    "        \"\"\"  \n",
    "\n",
    "        return 1-np.sum((y_pred-y_true)**2)/np.sum((y_true-np.mean(y_true))**2)\n",
    "    \n",
    "    def kge(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the KGE loss\n",
    "        \n",
    "        Parameters:\n",
    "            y_pred, array:\n",
    "                prediction results\n",
    "            y_true, array:\n",
    "                actual results\n",
    "        \"\"\" \n",
    "\n",
    "        kge_r = np.corrcoef(y_true,y_pred)[1][0]\n",
    "        kge_a = np.std(y_pred)/np.std(y_true)\n",
    "        kge_b = np.mean(y_pred)/np.mean(y_true)\n",
    "        return 1-np.sqrt((kge_r-1)**2+(kge_a-1)**2+(kge_b-1)**2)\n",
    "    \n",
    "    \n",
    "    RESULT_PATH = resultDir\n",
    "    os.mkdir(RESULT_PATH + station_id)\n",
    "    testname = RESULT_PATH + station_id + '/' + station_id\n",
    "    \n",
    "    DATASET_PATH = dataAsBinary\n",
    "    \n",
    "    train_x = loadBinary(DATASET_PATH + station_id + \"_train_x.npy\")\n",
    "    train_y = loadBinary(DATASET_PATH + station_id + \"_train_y.npy\")\n",
    "    test_x = loadBinary(DATASET_PATH + station_id + \"_test_x.npy\")\n",
    "    test_y = loadBinary(DATASET_PATH + station_id + \"_test_y.npy\")\n",
    "    \n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_x.fit(train_x)\n",
    "    scaler_y.fit(train_y)\n",
    "    \n",
    "    train_x_scaled = scaler_x.transform(train_x)\n",
    "    train_y_scaled = scaler_y.transform(train_y)\n",
    "    test_x_scaled = scaler_x.transform(test_x)\n",
    "\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(train_x_scaled, train_y_scaled, test_size=0.2, shuffle= False)\n",
    "    \n",
    "\n",
    "    x_train_encoder = x_train[:,:72*3].reshape(-1, 72, 3)\n",
    "    x_train_decoder = x_train[:,72*3:].reshape(-1, 120, 2)\n",
    "    x_valid_encoder = x_valid[:,:72*3].reshape(-1, 72, 3)\n",
    "    x_valid_decoder = x_valid[:,72*3:].reshape(-1, 120, 2)\n",
    "    x_test_encoder = test_x_scaled[:,:72*3].reshape(-1, 72, 3)\n",
    "    x_test_decoder = test_x_scaled[:,72*3:].reshape(-1, 120, 2)\n",
    "\n",
    "    \n",
    "    batch_size = 32\n",
    "    lr = 0.0001\n",
    "    epochs = 300\n",
    "    dim_dense=[128, 64, 64, 32, 32]\n",
    "    drop=0.20\n",
    "\n",
    "    encoder_input = Input(shape=(72,3))\n",
    "    encoder_1 = GRU(32, return_state=True, return_sequences=True)\n",
    "    encoder_output1, encoder_hc1 = encoder_1(encoder_input)\n",
    "    encoder_2 = GRU(32, return_state=True, return_sequences=True)\n",
    "    encoder_output2, encoder_hc2 = encoder_2(encoder_output1)\n",
    "    encoder_3 = GRU(32, return_state=True, return_sequences=True)\n",
    "    encoder_output3, encoder_hc3 = encoder_3(encoder_output2)\n",
    "    encoder_4 = GRU(32, return_state=True, return_sequences=True)\n",
    "    encoder_output4, encoder_hc4 = encoder_4(encoder_output3)\n",
    "    encoder_5 = GRU(32, return_state=True, return_sequences=True)\n",
    "    encoder_output5, encoder_hc5 = encoder_5(encoder_output4)\n",
    " \n",
    "    decoder_input = Input(shape=(120,2))\n",
    "    decoder_1 = GRU(32, return_sequences=True)\n",
    "    decoder_2 = GRU(32, return_sequences=True)\n",
    "    decoder_3 = GRU(32, return_sequences=True)\n",
    "    decoder_4 = GRU(32, return_sequences=True)\n",
    "    decoder_5 = GRU(32, return_sequences=True)\n",
    "\n",
    "    x = decoder_1(decoder_input, initial_state=encoder_hc1)\n",
    "    x = decoder_2(x, initial_state=encoder_hc2)\n",
    "    x = decoder_3(x, initial_state=encoder_hc3)\n",
    "    x = decoder_4(x, initial_state=encoder_hc4)\n",
    "    x = decoder_5(x, initial_state=encoder_hc5)\n",
    " \n",
    "\n",
    "    for dim in dim_dense:\n",
    "        x = TimeDistributed(Dense(dim, activation='relu'))(x)\n",
    "        x = TimeDistributed(Dropout(drop))(x)\n",
    "    main_out = TimeDistributed(Dense(1, activation='linear'))(x)\n",
    "    main_out = Flatten()(main_out)\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=main_out)\n",
    "    \n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                      patience=10, cooldown=200, min_lr=1e-8)\n",
    "    earlystoping = EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                            patience=20, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(testname+'_model.h5', monitor='val_loss', verbose=1,\n",
    "                    save_best_only=True, mode='min')\n",
    "    RMSprop=keras.optimizers.RMSprop(lr=lr)\n",
    "    \n",
    "    model.compile(optimizer=RMSprop, loss='mse')\n",
    "    model.summary()\n",
    "\n",
    "   \n",
    "    history = model.fit([x_train_encoder, x_train_decoder], y_train, epochs=epochs, batch_size=batch_size,\n",
    "              validation_data=([x_valid_encoder, x_valid_decoder], y_valid), callbacks=[reduce_lr, earlystoping, checkpoint], verbose=1)\n",
    "\n",
    "    \n",
    "    loss1 = history.history['loss']\n",
    "    loss2 = history.history['val_loss']\n",
    "    loss1 = pd.DataFrame({'TrainLoss':loss1})\n",
    "    loss2 = pd.DataFrame({'TestLoss':loss2})\n",
    "    LossEpoches=pd.concat([loss1, loss2], axis=1)\n",
    "    LossName = testname + '_loss.csv'\n",
    "    LossEpoches.to_csv(LossName, index = True)\n",
    "    \n",
    "    \n",
    "    model.load_weights(testname+'_model.h5')\n",
    "    \n",
    "    # Training is finished\n",
    "    \n",
    "    train_x = loadBinary(DATASET_PATH + station_id + \"_train_x.npy\")\n",
    "    train_y = loadBinary(DATASET_PATH + station_id + \"_train_y.npy\")\n",
    "    test_x = loadBinary(DATASET_PATH + station_id + \"_test_x.npy\")\n",
    "    test_y = loadBinary(DATASET_PATH + station_id + \"_test_y.npy\")\n",
    "\n",
    "    train_x = scaler_x.transform(train_x)\n",
    "    test_x = scaler_x.transform(test_x)\n",
    "\n",
    "    x_test_encoder = test_x[:,:72*3].reshape(-1, 72, 3)\n",
    "    x_test_decoder = test_x[:,72*3:].reshape(-1, 120, 2)\n",
    "    x_train_encoder = train_x[:,:72*3].reshape(-1, 72, 3)\n",
    "    x_train_decoder = train_x[:,72*3:].reshape(-1, 120, 2)\n",
    "\n",
    "    y_model_scaled_test = model.predict([x_test_encoder,x_test_decoder])\n",
    "    y_model_test = scaler_y.inverse_transform(y_model_scaled_test)\n",
    "\n",
    "    y_model_scaled_train = model.predict([x_train_encoder,x_train_decoder])\n",
    "    y_model_train = scaler_y.inverse_transform(y_model_scaled_train)\n",
    "\n",
    "    NSEs_train=[]\n",
    "    KGEs_train=[]\n",
    "    NSEs_test=[]\n",
    "    KGEs_test=[]\n",
    "    for x in range(0, 120):\n",
    "        y_pred_test = y_model_test[:,x]\n",
    "        y_True_test = test_y[:,x]\n",
    "\n",
    "        y_pred_train = y_model_train[:,x]\n",
    "        y_True_train = train_y[:,x]\n",
    "\n",
    "        NSEs_test.append(nse(y_True_test, y_pred_test))\n",
    "        KGEs_test.append(kge(y_True_test, y_pred_test))\n",
    "        NSEs_train.append(nse(y_True_train, y_pred_train))\n",
    "        KGEs_train.append(kge(y_True_train, y_pred_train))\n",
    "\n",
    "    updateJSON(station_id, NSEs_train, NSEs_test, KGEs_train, KGEs_test, \"summary_file_path\")\n",
    "    NSEs_test=pd.DataFrame(NSEs_test)\n",
    "    NSEs_test.columns = ['NSE_Test']\n",
    "    KGEs_test=pd.DataFrame(KGEs_test)\n",
    "    KGEs_test.columns = ['KGE_Test']\n",
    "    NSEs_train=pd.DataFrame(NSEs_train)\n",
    "    NSEs_train.columns = ['NSE_Train']\n",
    "    KGEs_train=pd.DataFrame(KGEs_train)\n",
    "    KGEs_train.columns = ['KGE_Train']\n",
    "    \n",
    "    eva = pd.concat([NSEs_test, KGEs_test, NSEs_train, KGEs_train], axis=1)\n",
    "    name_eva = testname + '_eva.csv'\n",
    "    eva.to_csv(name_eva, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAsBinary = binaryData # unzip the files in binaryData directory and gave as dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = glob.glob(dataAsBinary +'*')\n",
    "sensors = [i.split(\"_\")[0] for i in l]\n",
    "sensors = list(set(sensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createSummaryFile(\"summary_file_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor in sensors:\n",
    "    Seg2Seg_main(sensor, resultDir, dataAsBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
