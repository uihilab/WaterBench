{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from models import BiGRU as GRU\n",
    "import os\n",
    "import json\n",
    "from metrics import nse, kge\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.utils.data as uitilsData\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "class BenchMarkDataset(uitilsData.Dataset):\n",
    "    \"\"\"\n",
    "    Prepare the dataset or a gauge for the models\n",
    "    \n",
    "    Parameters:\n",
    "        path, str:\n",
    "            Path to dataset which contains train and test set for each gauge as csv files\n",
    "        sensorID, int:\n",
    "            id of prepared gauge\n",
    "        split, str:\n",
    "            identify whether train or test set will be prepared\n",
    "        gpu, int:\n",
    "            id of used gpu\n",
    "        scaler, obj:\n",
    "            minmax scaler which is created based on train set\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, sensorID=None, split=\"train\", gpu=0, scaler=(None, None)):\n",
    "\n",
    "        self.path = path\n",
    "        self.split = split\n",
    "        self.gpu = gpu\n",
    "        self.sensorID = sensorID\n",
    "\n",
    "        self.scalerX = scaler[0]\n",
    "        self.scalerY = scaler[1]\n",
    "        \n",
    "        self.read()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx,:,:].to(torch.device('cuda:%d' % self.gpu))\n",
    "        y = self.y[idx,:].to(torch.device('cuda:%d' % self.gpu))\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def get_values(self):\n",
    "        \n",
    "        return (self.scalerX, self.scalerY)\n",
    "\n",
    "    def read_file(self, file_path):\n",
    "        X_path = file_path + \"_x.csv\"\n",
    "        y_path = file_path + \"_y.csv\"\n",
    "        X = pd.read_csv(X_path, index_col='datetime')\n",
    "        y = pd.read_csv(y_path, index_col='datetime')\n",
    "\n",
    "        X = X.values[:,:-7]\n",
    "        y = y.values\n",
    "        x_train_history = X[:,:72*3].reshape(-1, 72, 3)\n",
    "        x_train_future = X[:,72*3:].reshape(-1, 120, 2)\n",
    "        x_train_future = x_train_future[:,:,[0,1,1]]\n",
    "        x_train_future[:,:,2] = 0\n",
    "        ds_X = np.concatenate([x_train_history,x_train_future],axis=1)\n",
    "\n",
    "        return ds_X, y\n",
    "\n",
    "    def read(self):\n",
    "\n",
    "        ds_X = np.random.rand(1, 192, 3)\n",
    "        ds_y = np.random.rand(1, 120)\n",
    "        \n",
    "        l = glob.glob(self.path+'*')\n",
    "        sensors = [i.split(\"_\")[0] for i in l]\n",
    "        sensors = list(set(sensors))\n",
    "        \n",
    "        if self.sensorID != None:\n",
    "            file_path = self.path + str(self.sensorID) + \"_\" + self.split\n",
    "            X, y = self.read_file(file_path)\n",
    "            ds_X = np.concatenate((ds_X, X), 0) \n",
    "            ds_y = np.concatenate((ds_y, y), 0)\n",
    "        else:\n",
    "            for sensor in sensors:\n",
    "                file_path =  sensor + \"_\" + self.split\n",
    "                X, y = self.read_file(file_path)\n",
    "                ds_X = np.concatenate((ds_X, X), 0) \n",
    "                ds_y = np.concatenate((ds_y, y), 0)\n",
    "        \n",
    "        ds_X = np.delete(ds_X, 0, 0)\n",
    "        ds_y = np.delete(ds_y, 0, 0)\n",
    "\n",
    "        shapeX = ds_X.shape\n",
    "        \n",
    "        \n",
    "        if self.split == \"train\":\n",
    "\n",
    "            self.scalerX = MinMaxScaler()\n",
    "            self.scalerY = MinMaxScaler()\n",
    "            \n",
    "            self.scalerX.fit(ds_X.reshape((shapeX[0], shapeX[1] * shapeX[2])))\n",
    "            self.scalerY.fit(ds_y)\n",
    "\n",
    "        \n",
    "        ds_X = self.scalerX.transform(ds_X.reshape((shapeX[0], shapeX[1] * shapeX[2])))\n",
    "        ds_X = ds_X.reshape(shapeX)\n",
    "        ds_y = self.scalerY.transform(ds_y)\n",
    "\n",
    "        ds_X = torch.Tensor(ds_X)\n",
    "        ds_y = torch.Tensor(ds_y)\n",
    "        self.X = ds_X\n",
    "        self.y = ds_y.unsqueeze(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSummaryFile(filename):\n",
    "    \"\"\"\n",
    "    Create a summary file for the results of each gauge\n",
    "        \n",
    "    Parameters:\n",
    "        filename, str:\n",
    "            filename(path) of the summary file\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Results = {}\n",
    "    Results[\"Train\"] = {}\n",
    "    Results[\"Test\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"][\"max\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"][\"min\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"][\"median\"] = {}\n",
    "    Results[\"Train\"][\"NSE\"][\"mean\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"][\"max\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"][\"min\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"][\"median\"] = {}\n",
    "    Results[\"Train\"][\"KGE\"][\"mean\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"][\"max\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"][\"min\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"][\"median\"] = {}\n",
    "    Results[\"Test\"][\"NSE\"][\"mean\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"][\"max\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"][\"min\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"][\"median\"] = {}\n",
    "    Results[\"Test\"][\"KGE\"][\"mean\"] = {}\n",
    "    \n",
    "    with open(filename, \"w\") as outfile:\n",
    "        json.dump(Results, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateJSON(station_id, NSE_train, NSE_test, KGE_train, KGE_test, jsonFilePath):\n",
    "    \"\"\"\n",
    "    Updates the summary file with results of a gauge\n",
    "    \n",
    "    Parameters:\n",
    "        station_id, int:\n",
    "            id of the gauge which results will be added to summary file\n",
    "        NSE_train, list:\n",
    "            NSE scores of train set\n",
    "        NSE_test, list:\n",
    "            NSE scores of test set\n",
    "        KGE_train, list:\n",
    "            KGE scores of train set\n",
    "        KGE_test, list:\n",
    "            KGE scores of test set\n",
    "        jsonFilePath, str:\n",
    "            path of the updated summary file\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    nse_train_max = np.max(NSE_train)\n",
    "    nse_train_min = np.min(NSE_train)\n",
    "    nse_train_median = np.median(NSE_train)\n",
    "    nse_train_mean = np.mean(NSE_train)\n",
    "    \n",
    "    nse_test_max = np.max(NSE_test)\n",
    "    nse_test_min = np.min(NSE_test)\n",
    "    nse_test_median = np.median(NSE_test)\n",
    "    nse_test_mean = np.mean(NSE_test)\n",
    "    \n",
    "    kge_train_max = np.max(KGE_train)\n",
    "    kge_train_min = np.min(KGE_train)\n",
    "    kge_train_median = np.median(KGE_train)\n",
    "    kge_train_mean = np.mean(KGE_train)\n",
    "    \n",
    "    kge_test_max = np.max(KGE_test)\n",
    "    kge_test_min = np.min(KGE_test)\n",
    "    kge_test_median = np.median(KGE_test)\n",
    "    kge_test_mean = np.mean(KGE_test)\n",
    "    \n",
    "    with open(jsonFilePath, \"r\") as jsonFile:\n",
    "        Results = json.load(jsonFile)\n",
    "    \n",
    "    Results[\"Train\"][\"NSE\"][\"max\"][station_id] = nse_train_max\n",
    "    Results[\"Train\"][\"NSE\"][\"min\"][station_id] = nse_train_min\n",
    "    Results[\"Train\"][\"NSE\"][\"median\"][station_id] = nse_train_median\n",
    "    Results[\"Train\"][\"NSE\"][\"mean\"][station_id] = nse_train_mean\n",
    "    \n",
    "    Results[\"Train\"][\"KGE\"][\"max\"][station_id] = kge_train_max\n",
    "    Results[\"Train\"][\"KGE\"][\"min\"][station_id] = kge_train_min\n",
    "    Results[\"Train\"][\"KGE\"][\"median\"][station_id] = kge_train_median\n",
    "    Results[\"Train\"][\"KGE\"][\"mean\"][station_id] = kge_train_mean\n",
    "    \n",
    "    Results[\"Test\"][\"NSE\"][\"max\"][station_id] = nse_test_max\n",
    "    Results[\"Test\"][\"NSE\"][\"min\"][station_id] = nse_test_min\n",
    "    Results[\"Test\"][\"NSE\"][\"median\"][station_id] = nse_test_median\n",
    "    Results[\"Test\"][\"NSE\"][\"mean\"][station_id] = nse_test_mean\n",
    "    \n",
    "    Results[\"Test\"][\"KGE\"][\"max\"][station_id] = kge_test_max\n",
    "    Results[\"Test\"][\"KGE\"][\"min\"][station_id] = kge_test_min\n",
    "    Results[\"Test\"][\"KGE\"][\"median\"][station_id] = kge_test_median\n",
    "    Results[\"Test\"][\"KGE\"][\"mean\"][station_id] = kge_test_mean\n",
    "    \n",
    "    with open(jsonFilePath, \"w\") as jsonFile:\n",
    "        json.dump(Results, jsonFile)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_main(summaryFilePath, datasetPath, resultsPath, gpu=0):\n",
    "    \"\"\"\n",
    "    Main function to get results of GRU model. It should be noted that\n",
    "    batch size, learning rate or number of epochs can be change with updating\n",
    "    corresponding values.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        summaryFilePath, str:\n",
    "            filename(path) of the summary file\n",
    "        datasetPath, str:\n",
    "            Path to dataset which contains train and test set for each gauge as csv files\n",
    "        resultsPath, str:\n",
    "            Path to save results of each gauge\n",
    "        gpu, int:\n",
    "            id of used gpu\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    summaryFile = summaryFilePath\n",
    "    \n",
    "    DATASET_PATH = datasetPath\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 0\n",
    "    LR = 1e-4\n",
    "    EPOCHS = 50\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_ = BenchMarkDataset(DATASET_PATH, sensorID=None, split='train', gpu=gpu)\n",
    "    trainloader = torch.utils.data.DataLoader(train_, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    test_ = BenchMarkDataset(DATASET_PATH, sensorID=None, split='test', gpu=gpu, scaler=train_.get_values())\n",
    "    testloader = torch.utils.data.DataLoader(test_, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    INPUT_DIM = 3\n",
    "    HIDDEN_DIM = 64\n",
    "    OUTPUT_DIM = 1\n",
    "    NUM_LAYERS = 2\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    netGRU = GRU(\n",
    "        INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS, DROPOUT)\n",
    "    netGRU.to(torch.device('cuda:%d' % gpu))\n",
    "    \n",
    "    optimizer = optim.Adam(netGRU.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    L1Loss = torch.nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    valid_model_path = \"valid.pth\"\n",
    "    val_loss_best = np.inf\n",
    "\n",
    "    hist_loss = np.zeros(EPOCHS)\n",
    "    hist_loss_val = np.zeros(EPOCHS)\n",
    "    \n",
    "\n",
    "    for idx_epoch in range(EPOCHS):\n",
    "        running_loss = 0\n",
    "        for idx_batch, (x, y) in enumerate(trainloader, 1):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = netGRU(x)\n",
    "            minibatch_loss = L1Loss(y, y_hat[:, -120:])\n",
    "            minibatch_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += minibatch_loss.item()\n",
    "\n",
    "        train_loss = running_loss/len(trainloader)\n",
    "        running_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in testloader:\n",
    "                y_hat = netGRU(x)\n",
    "                running_loss += L1Loss(y, y_hat[:, -120:])\n",
    "\n",
    "        val_loss = (running_loss / len(testloader))\n",
    "        scheduler.step(train_loss)\n",
    "        val_loss = val_loss.item()\n",
    "\n",
    "        hist_loss[idx_epoch] = train_loss\n",
    "        hist_loss_val[idx_epoch] = val_loss\n",
    "\n",
    "        if val_loss < val_loss_best:\n",
    "            val_loss_best = val_loss\n",
    "            torch.save(netGRU.state_dict(), valid_model_path)\n",
    "    \n",
    "    netGRU.load_state_dict(torch.load(valid_model_path))\n",
    "\n",
    "    l = os.listdir(DATASET_PATH)\n",
    "    sensors = [i.split(\"_\")[0] for i in l]\n",
    "    sensors = list(set(sensors))\n",
    "    sensors = sorted(sensors, reverse=True)\n",
    "    for sensorID in sensors:\n",
    "        sensorID = int(sensorID)\n",
    "        sensorDirectory = resultsPath + \"%d\" % (sensorID)\n",
    "        os.mkdir(sensorDirectory)\n",
    "        train_ = BenchMarkDataset(DATASET_PATH, sensorID, split='train', gpu=gpu)\n",
    "        trainloader = torch.utils.data.DataLoader(train_, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "        test_ = BenchMarkDataset(DATASET_PATH, sensorID, split='test', gpu=gpu, scaler=train_.get_values())\n",
    "        testloader = torch.utils.data.DataLoader(test_, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for k, (x, y) in enumerate(trainloader, 1):\n",
    "                output = netGRU(x)\n",
    "                if k == 1:\n",
    "                    result_pred_train = output\n",
    "                    result_train = y\n",
    "                else:\n",
    "                    result_pred_train = torch.cat((result_pred_train, output), 0)\n",
    "                    result_train = torch.cat((result_train, y), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k, (x, y) in enumerate(testloader, 1):\n",
    "                output = netGRU(x)\n",
    "                if k == 1:\n",
    "                    result_pred_test = output\n",
    "                    result_test = y\n",
    "                else:\n",
    "                    result_pred_test = torch.cat((result_pred_test, output), 0)\n",
    "                    result_test = torch.cat((result_test, y), 0)\n",
    "    \n",
    "        shapeOutputTest = result_test.shape\n",
    "        shapeOutputTrain = result_train.shape\n",
    "\n",
    "        result_pred_train = train_.scalerY.inverse_transform(result_pred_train.cpu().numpy()[:, -120:].reshape((shapeOutputTrain[0], 120)))\n",
    "        result_train = train_.scalerY.inverse_transform(result_train.cpu().numpy().reshape((shapeOutputTrain[0], 120)))\n",
    "\n",
    "        result_pred_test = train_.scalerY.inverse_transform(result_pred_test.cpu().numpy()[:, -120:].reshape((shapeOutputTest[0], 120)))\n",
    "        result_test = train_.scalerY.inverse_transform(result_test.cpu().numpy().reshape((shapeOutputTest[0], 120)))\n",
    "    \n",
    "        NSEs_train = []\n",
    "        NSEs_test = []\n",
    "        KGEs_train = []\n",
    "        KGEs_test = []\n",
    "    \n",
    "        for i in range(120):\n",
    "            NSEs_train.append(nse(result_train[:, i], result_pred_train[:, i]))\n",
    "            NSEs_test.append(nse(result_test[:, i], result_pred_test[:, i]))\n",
    "            KGEs_train.append(kge(result_train[:, i], result_pred_train[:, i]))\n",
    "            KGEs_test.append(kge(result_test[:, i], result_pred_test[:, i]))\n",
    "    \n",
    "        updateJSON(sensorID, NSEs_train, NSEs_test, KGEs_train, KGEs_test, summaryFile)\n",
    "    \n",
    "        KGE_train = pd.DataFrame(KGEs_train)\n",
    "        KGE_test = pd.DataFrame(KGEs_test)\n",
    "        KGE_train.columns = [\"KGEsTrain\"]\n",
    "        KGE_test.columns = [\"KGEsTest\"]\n",
    "        NSE_train = pd.DataFrame(NSEs_train)\n",
    "        NSE_test = pd.DataFrame(NSEs_test)\n",
    "        NSE_train.columns = [\"NSEsTrain\"]\n",
    "        NSE_test.columns = [\"NSEsTest\"]\n",
    "    \n",
    "    \n",
    "        combined = pd.concat([NSE_train, NSE_test, KGE_train, KGE_test], axis=1)\n",
    "        combined.to_csv(\"%s/%s.csv\" % (sensorDirectory, str(sensorID)), index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "createSummaryFile(summaryFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPath = originalData # unzip the files in originalData folder and gave as dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_main(summaryFilePath, datasetPath, resultsPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
